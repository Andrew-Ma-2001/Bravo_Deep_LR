{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6895d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install resnest\n",
    "# !pip install pretrainedmodels\n",
    "# !pip install geffnet\n",
    "# !pip install apex\n",
    "# !pip install -user albumentations \n",
    "# !pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f3fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sky31\\anaconda3\\lib\\site-packages\\apex-0.1-py3.8.egg\\apex\\pyprof\\__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import geffnet\n",
    "from resnest.torch import resnest101\n",
    "from pretrainedmodels import se_resnext101_32x4d\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import apex\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13bbeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "class Swish(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "\n",
    "class Swish_Module(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return Swish.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db1ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Effnet_Melanoma(nn.Module):\n",
    "    def __init__(self, enet_type, out_dim, n_meta_features=0, n_meta_dim=[512, 128], pretrained=False):\n",
    "        super(Effnet_Melanoma, self).__init__()\n",
    "        self.n_meta_features = n_meta_features\n",
    "        self.enet = geffnet.create_model(enet_type, pretrained=pretrained)\n",
    "        self.dropouts = nn.ModuleList([\n",
    "            nn.Dropout(0.5) for _ in range(5)\n",
    "        ])\n",
    "        in_ch = self.enet.classifier.in_features\n",
    "        if n_meta_features > 0:\n",
    "            self.meta = nn.Sequential(\n",
    "                nn.Linear(n_meta_features, n_meta_dim[0]),\n",
    "                nn.BatchNorm1d(n_meta_dim[0]),\n",
    "                Swish_Module(),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(n_meta_dim[0], n_meta_dim[1]),\n",
    "                nn.BatchNorm1d(n_meta_dim[1]),\n",
    "                Swish_Module(),\n",
    "            )\n",
    "            in_ch += n_meta_dim[1]\n",
    "        self.myfc = nn.Linear(in_ch, out_dim)\n",
    "        self.enet.classifier = nn.Identity()\n",
    "\n",
    "    def extract(self, x):\n",
    "        x = self.enet(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, x_meta=None):\n",
    "        x = self.extract(x).squeeze(-1).squeeze(-1)\n",
    "        if self.n_meta_features > 0:\n",
    "            x_meta = self.meta(x_meta)\n",
    "            x = torch.cat((x, x_meta), dim=1)\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                out = self.myfc(dropout(x))\n",
    "            else:\n",
    "                out += self.myfc(dropout(x))\n",
    "        out /= len(self.dropouts)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9aba3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    \n",
    "    kernel_type = \"9c_b4ns_448_ext_15ep-newfold\"\n",
    "    data_dir = \"./data/\"\n",
    "    data_folder = 512\n",
    "    image_size = 448\n",
    "    enet_type = \"tf_efficientnet_b4_ns\"\n",
    "    batch_size = 20\n",
    "    num_workers = 0\n",
    "    init_lr = 0.0001\n",
    "    out_dim = 6\n",
    "    n_epochs = 5\n",
    "    model_dir = './weights'\n",
    "    CUDA_VISIBLE_DEVICES = '0'\n",
    "    fold = '0,1,2,3,4'\n",
    "    n_meta_dim = '512,128'\n",
    "    image_folder = \"ISIC2019_images\"\n",
    "    \n",
    "    DEBUG = False\n",
    "    log_dir = \"./logs\"\n",
    "    use_meta = False\n",
    "    use_amp = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f5c42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        \n",
    "        # print(data.shape)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if args.use_meta:\n",
    "            data, meta = data\n",
    "            data, meta, target = data.to(device), meta.to(device), target.to(device)\n",
    "            logits = model(data, meta)\n",
    "        else:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)        \n",
    "        \n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        if not args.use_amp:\n",
    "            loss.backward()\n",
    "        else:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "        if args.image_size in [896,576]:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "\n",
    "    train_loss = np.mean(train_loss)\n",
    "    return train_loss\n",
    "\n",
    "def val_epoch(model, loader, mel_idx, is_ext=None, n_test=1, get_output=False):\n",
    "\n",
    "    model.eval()\n",
    "    predict = []\n",
    "    TARGETS = []\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(1)\n",
    "            TARGETS += list(target.cpu().numpy())\n",
    "            predict += list(pred.cpu().numpy())\n",
    "            loss = criterion(out, target)\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "        acc = (np.array(predict) == np.array(TARGETS)).mean() * 100.\n",
    "        print(classification_report(TARGETS, predict, target_names = [\"MEL\", \"NV\", \"BCC\", \"BKL\", \"SCC\", \"AK_DF_VASC\"]))\n",
    "        return val_loss, acc\n",
    "\n",
    "def run(fold, df, meta_features, n_meta_features, transforms_train, transforms_val, mel_idx):\n",
    "    \n",
    "    df_train = df[df[\"train_fold\"] != fold]\n",
    "    df_valid = df[df[\"val_fold\"] == fold]\n",
    "\n",
    "    dataset_train = MelanomaDataset(df_train, 'train', meta_features, transform=transforms_train)\n",
    "    dataset_valid = MelanomaDataset(df_valid, 'valid', meta_features, transform=transforms_val)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, sampler=RandomSampler(dataset_train), num_workers=0)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=args.batch_size, num_workers=0)\n",
    "    \n",
    "    if args.enet_type == 'resnest101':\n",
    "        model = Resnest_Melanoma(\n",
    "        args.enet_type,\n",
    "        n_meta_features=n_meta_features,\n",
    "        n_meta_dim=[int(nd) for nd in args.n_meta_dim.split(',')],\n",
    "        out_dim=args.out_dim,\n",
    "        pretrained=True\n",
    "    )\n",
    "    elif args.enet_type == 'seresnext101':\n",
    "        model = Seresnext_Melanoma(\n",
    "        args.enet_type,\n",
    "        n_meta_features=n_meta_features,\n",
    "        n_meta_dim=[int(nd) for nd in args.n_meta_dim.split(',')],\n",
    "        out_dim=args.out_dim,\n",
    "        pretrained=True\n",
    "    )\n",
    "    elif 'efficientnet' in args.enet_type:\n",
    "        model = Effnet_Melanoma(\n",
    "        args.enet_type,\n",
    "        n_meta_features=n_meta_features,\n",
    "        n_meta_dim=[int(nd) for nd in args.n_meta_dim.split(',')],\n",
    "        out_dim=args.out_dim,\n",
    "        pretrained=True\n",
    "    )\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    # print(model)\n",
    "\n",
    "    auc_max = 0.\n",
    "    model_file  = os.path.join(args.model_dir, f'{args.kernel_type}_best.pth')\n",
    "    model_file3 = os.path.join(args.model_dir, f'{args.kernel_type}_final.pth')\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.init_lr)\n",
    "    if args.use_amp:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "    if DP:\n",
    "        model = nn.DataParallel(model)\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.n_epochs - 1)\n",
    "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "    \n",
    "    print(len(dataset_train), len(dataset_valid))\n",
    "\n",
    "    for epoch in range(1, args.n_epochs + 1):\n",
    "        print(time.ctime(), f'Epoch {epoch}')\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss, acc = val_epoch(model, valid_loader, mel_idx, is_ext=df_valid['is_ext'].values)\n",
    "\n",
    "        content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {(val_loss):.5f}, acc: {(acc):.4f}.'\n",
    "        print(content)\n",
    "        with open(os.path.join(args.log_dir, f'log_{args.kernel_type}.txt'), 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "\n",
    "        scheduler_warmup.step()    \n",
    "        if epoch==2: scheduler_warmup.step()\n",
    "            \n",
    "    torch.save(model.state_dict(), model_file3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb9807e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18556698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelanomaDataset(Dataset):\n",
    "    def __init__(self, csv, mode, meta_features, transform=None):\n",
    "    \n",
    "        self.csv = csv.reset_index(drop=True)\n",
    "        self.target = list(csv[\"target\"])\n",
    "        self.image = list(csv[\"filepath\"])\n",
    "        self.mode = mode\n",
    "        self.use_meta = meta_features is not None\n",
    "        self.meta_features = meta_features\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = np.load(self.image[index][:-4] + \".npy\")\n",
    "        data = torch.tensor(image).float()\n",
    "#         row = self.csv.iloc[index]\n",
    "#         image = cv2.imread(row.filepath)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             res = self.transform(image=image)\n",
    "#             image = res['image'].astype(np.float32)\n",
    "#         else:\n",
    "#             image = image.astype(np.float32)\n",
    "\n",
    "#         image = image.transpose(2, 0, 1)\n",
    "\n",
    "#         if self.use_meta:\n",
    "#             data = (torch.tensor(image).float(), torch.tensor(self.csv.iloc[index][self.meta_features]).float())\n",
    "#         else:\n",
    "#             data = torch.tensor(image).float()\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return data\n",
    "        else:\n",
    "            return data, torch.tensor(self.target[index]).long()\n",
    "        \n",
    "def get_transforms(image_size):\n",
    "\n",
    "    transforms_train = albumentations.Compose([\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.RandomBrightness(limit=0.2, p=0.75),\n",
    "        albumentations.RandomContrast(limit=0.2, p=0.75),\n",
    "        albumentations.OneOf([\n",
    "            albumentations.MotionBlur(blur_limit=5),\n",
    "            albumentations.MedianBlur(blur_limit=5),\n",
    "            albumentations.GaussianBlur(blur_limit=5),\n",
    "            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "        ], p=0.7),\n",
    "\n",
    "        albumentations.OneOf([\n",
    "            albumentations.OpticalDistortion(distort_limit=1.0),\n",
    "            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "            albumentations.ElasticTransform(alpha=3),\n",
    "        ], p=0.7),\n",
    "\n",
    "        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n",
    "        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "        albumentations.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    transforms_val = albumentations.Compose([\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val\n",
    "\n",
    "def get_df(kernel_type, out_dim, data_dir, data_folder, use_meta):\n",
    "\n",
    "    # 2019 csv data \n",
    "    df_train2 = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    df_train2 = df_train2.drop(\"UNK\", 1)\n",
    "    df_train2[\"AK_DF_VASC\"] = df_train2[\"AK\"] + df_train2[\"DF\"] + df_train2[\"VASC\"]\n",
    "    df_train2 = df_train2.drop([\"AK\", \"DF\",\"VASC\"], 1)\n",
    "    \n",
    "    # index - 3, 5, 6\n",
    "    # AK, DF, VASC\n",
    "    diagnosis2idx = {}\n",
    "    df_train2.columns[1:]\n",
    "    for i in df_train2.columns[1:]:\n",
    "        diagnosis2idx[i] = len(diagnosis2idx)\n",
    "    idx2diagnosis = {v:k for k, v in diagnosis2idx.items()}\n",
    "\n",
    "    idx = np.argmax(np.array(df_train2)[:, 1:], axis = 1)\n",
    "    df_train2['target']  = idx\n",
    "    df_train2['diagnosis'] = df_train2['target'].map(idx2diagnosis)\n",
    "    df_train2['filepath'] = df_train2['image'].apply(lambda x: os.path.join(\"data\", args.image_folder, f'{x}.jpg'))\n",
    "    \n",
    "    df_train2['tfrecord'] = list(range(len(idx)))\n",
    "    df_train2['train_fold'] = df_train2['tfrecord'] % 8\n",
    "    df_train2['val_fold'] = df_train2['tfrecord'] % 6\n",
    "    df_train2['is_ext'] = 1\n",
    "    \n",
    "    types = list(diagnosis2idx.keys())\n",
    "    samples = df_train2[df_train2[types[0]]==1].sample(600)\n",
    "    for tp in types[1:]:\n",
    "        tem = df_train2[df_train2[tp]==1].sample(600)\n",
    "        samples = samples.append(tem)\n",
    "    samples = samples.sort_values(\"image\")\n",
    "    \n",
    "    df_test = samples.sample(100)\n",
    "    meta_features = None\n",
    "    n_meta_features = 0\n",
    "\n",
    "    # class mapping\n",
    "    mel_idx = diagnosis2idx['MEL']\n",
    "\n",
    "    return samples, df_test, meta_features, n_meta_features, mel_idx, diagnosis2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c84ff61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warmup_scheduler import GradualWarmupScheduler \n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee0585fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2664bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.model_dir, exist_ok=True)\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.CUDA_VISIBLE_DEVICES\n",
    "\n",
    "DP = len(os.environ['CUDA_VISIBLE_DEVICES']) > 1\n",
    "\n",
    "set_seed()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64f37747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use transfer learning for ISIC 2019    \n",
    "df, df_test, meta_features, n_meta_features, mel_idx, diagnosis2idx = get_df(\n",
    "    args.kernel_type,\n",
    "    args.out_dim,\n",
    "    args.data_dir,\n",
    "    args.data_folder,\n",
    "    args.use_meta\n",
    ")\n",
    "\n",
    "transforms_train, transforms_val = get_transforms(args.image_size)\n",
    "fold = 0\n",
    "run(fold, df, meta_features, n_meta_features, transforms_train, transforms_val, mel_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "621fae93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3145 628\n",
      "Wed May 18 13:22:53 2022 Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.32193, smth: 1.43212: 100%|██████████████████████████████████████████████████| 158/158 [01:14<00:00,  2.13it/s]\n",
      "  0%|                                                                                          | 0/158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.51      0.48      0.50        95\n",
      "          NV       0.59      0.84      0.69       113\n",
      "         BCC       0.72      0.46      0.56       120\n",
      "         BKL       0.62      0.51      0.56       107\n",
      "         SCC       0.48      0.64      0.55        94\n",
      "  AK_DF_VASC       0.55      0.48      0.52        99\n",
      "\n",
      "    accuracy                           0.57       628\n",
      "   macro avg       0.58      0.57      0.56       628\n",
      "weighted avg       0.59      0.57      0.57       628\n",
      "\n",
      "Wed May 18 13:24:11 2022 Epoch 1, lr: 0.0001000, train loss: 1.54042, valid loss: 1.14749, acc: 57.1656.\n",
      "Wed May 18 13:24:11 2022 Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.55850, smth: 1.42668: 100%|██████████████████████████████████████████████████| 158/158 [01:08<00:00,  2.30it/s]\n",
      "  0%|                                                                                          | 0/158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.51      0.52      0.51        95\n",
      "          NV       0.68      0.69      0.68       113\n",
      "         BCC       0.59      0.46      0.52       120\n",
      "         BKL       0.49      0.37      0.43       107\n",
      "         SCC       0.46      0.69      0.55        94\n",
      "  AK_DF_VASC       0.43      0.43      0.43        99\n",
      "\n",
      "    accuracy                           0.53       628\n",
      "   macro avg       0.53      0.53      0.52       628\n",
      "weighted avg       0.53      0.53      0.52       628\n",
      "\n",
      "Wed May 18 13:25:25 2022 Epoch 2, lr: 0.0010000, train loss: 1.45264, valid loss: 1.27616, acc: 52.5478.\n",
      "Wed May 18 13:25:25 2022 Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.14609, smth: 1.16564: 100%|██████████████████████████████████████████████████| 158/158 [01:08<00:00,  2.29it/s]\n",
      "  0%|                                                                                          | 0/158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.52      0.36      0.42        95\n",
      "          NV       0.64      0.86      0.73       113\n",
      "         BCC       0.74      0.65      0.69       120\n",
      "         BKL       0.66      0.55      0.60       107\n",
      "         SCC       0.63      0.55      0.59        94\n",
      "  AK_DF_VASC       0.54      0.74      0.63        99\n",
      "\n",
      "    accuracy                           0.63       628\n",
      "   macro avg       0.62      0.62      0.61       628\n",
      "weighted avg       0.63      0.63      0.62       628\n",
      "\n",
      "Wed May 18 13:26:38 2022 Epoch 3, lr: 0.0008536, train loss: 1.17538, valid loss: 1.03590, acc: 62.5796.\n",
      "Wed May 18 13:26:38 2022 Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.63208, smth: 0.76081: 100%|██████████████████████████████████████████████████| 158/158 [01:10<00:00,  2.25it/s]\n",
      "  0%|                                                                                          | 0/158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.83      0.71      0.76        95\n",
      "          NV       0.81      0.90      0.85       113\n",
      "         BCC       0.79      0.89      0.84       120\n",
      "         BKL       0.85      0.76      0.80       107\n",
      "         SCC       0.75      0.78      0.76        94\n",
      "  AK_DF_VASC       0.80      0.76      0.78        99\n",
      "\n",
      "    accuracy                           0.80       628\n",
      "   macro avg       0.81      0.80      0.80       628\n",
      "weighted avg       0.81      0.80      0.80       628\n",
      "\n",
      "Wed May 18 13:27:53 2022 Epoch 4, lr: 0.0005000, train loss: 0.75688, valid loss: 0.66143, acc: 80.4140.\n",
      "Wed May 18 13:27:53 2022 Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.27291, smth: 0.27130: 100%|██████████████████████████████████████████████████| 158/158 [01:08<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.91      0.81      0.86        95\n",
      "          NV       0.85      0.94      0.89       113\n",
      "         BCC       0.89      0.90      0.90       120\n",
      "         BKL       0.90      0.86      0.88       107\n",
      "         SCC       0.82      0.82      0.82        94\n",
      "  AK_DF_VASC       0.91      0.93      0.92        99\n",
      "\n",
      "    accuracy                           0.88       628\n",
      "   macro avg       0.88      0.88      0.88       628\n",
      "weighted avg       0.88      0.88      0.88       628\n",
      "\n",
      "Wed May 18 13:29:06 2022 Epoch 5, lr: 0.0001464, train loss: 0.30915, valid loss: 0.45456, acc: 87.8981.\n"
     ]
    }
   ],
   "source": [
    "# 运行配置 \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fdb2f",
   "metadata": {},
   "source": [
    "## Third Part Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1f31ca94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_ISIC = Effnet_Melanoma(\n",
    "        args.enet_type,\n",
    "        n_meta_features=n_meta_features,\n",
    "        n_meta_dim=[int(nd) for nd in args.n_meta_dim.split(',')],\n",
    "        out_dim=args.out_dim,\n",
    "        pretrained=True\n",
    "    )\n",
    "\n",
    "pretrained_ISIC.load_state_dict(\n",
    "    torch.load(\"weights/9c_b4ns_448_ext_15ep-newfold_final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0f84b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDir(dirPath):\n",
    "    allFiles = []\n",
    "    if os.path.isdir(dirPath):\n",
    "        fileList = os.listdir(dirPath)\n",
    "        for f in fileList:\n",
    "            f = dirPath+'/'+f\n",
    "            if os.path.isdir(f):\n",
    "                subFiles = readDir(f)\n",
    "                allFiles = subFiles + allFiles\n",
    "            elif f.endswith(\"jpg\"):\n",
    "#                 if \"微信截图\" in f:\n",
    "#                     f2 = f.replace(\"微信截图_\", \"\")\n",
    "#                     os.rename(f, f2)\n",
    "#                     allFiles.append(f2)\n",
    "#                 else:\n",
    "                allFiles.append(f)\n",
    "        return allFiles\n",
    "    else:\n",
    "        return 'Error,not a dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "612cba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = readDir(\"OurData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "06dc6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for path in images:\n",
    "    class_ = path.split(\"/\")[1]\n",
    "    if \"df\" in class_:\n",
    "        labels.append(5)\n",
    "    else:\n",
    "        labels.append(diagnosis2idx[class_.upper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "96ee5119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "split = 10\n",
    "train_split = list(range(split)) * 200\n",
    "train_split = np.array(train_split[:len(images)])\n",
    "val_split = list(range(int(split/2))) * 200\n",
    "val_split = np.array(val_split[:len(images)])\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "886e6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 652/652 [00:59<00:00, 11.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# transfer the data into images\n",
    "for path in tqdm(images):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    res = transforms_train(image=image)\n",
    "    image = res['image'].astype(np.float32)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    # print(image.shape)\n",
    "    np.save(path[:-4]+\".npy\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e29c571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels, - train_split, val_split\n",
    "class ourDataset(Dataset):\n",
    "    def __init__(self, image, labels, transform=None):\n",
    "    \n",
    "        self.target = labels\n",
    "        self.image = image\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = np.load(self.image[index][:-4] + \".npy\")\n",
    "        data = torch.tensor(image).float()\n",
    "        return data, torch.tensor(self.target[index]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "17dc832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "train_image = images[train_split!=flag]\n",
    "val_image = images[val_split==flag]\n",
    "train_label = labels[train_split!=flag]\n",
    "val_label = labels[val_split==flag]\n",
    "\n",
    "train_data = ourDataset(train_image, train_label)\n",
    "val_data = ourDataset(val_image, val_label)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, sampler=RandomSampler(train_data), num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "231cb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.47      0.50      0.48        14\n",
      "          NV       0.23      0.25      0.24        20\n",
      "         BCC       0.59      0.21      0.31        47\n",
      "         BKL       0.06      0.50      0.10         4\n",
      "         SCC       0.11      0.08      0.10        12\n",
      "  AK_DF_VASC       0.44      0.41      0.42        34\n",
      "\n",
      "    accuracy                           0.30       131\n",
      "   macro avg       0.31      0.33      0.28       131\n",
      "weighted avg       0.42      0.30      0.32       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pretrained_ISIC.to(device)\n",
    "al_loss, acc = val_epoch(model, valid_loader, mel_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5d859753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 131\n",
      "Wed May 18 15:15:36 2022 Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.17930, smth: 1.30119: 100%|████████████████████████████████████████████████████| 30/30 [00:12<00:00,  2.44it/s]\n",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.91      0.71      0.80        14\n",
      "          NV       0.77      0.85      0.81        20\n",
      "         BCC       0.79      0.72      0.76        47\n",
      "         BKL       0.40      0.50      0.44         4\n",
      "         SCC       0.67      0.33      0.44        12\n",
      "  AK_DF_VASC       0.64      0.82      0.72        34\n",
      "\n",
      "    accuracy                           0.73       131\n",
      "   macro avg       0.70      0.66      0.66       131\n",
      "weighted avg       0.74      0.73      0.72       131\n",
      "\n",
      "Wed May 18 15:15:49 2022 Epoch 1, lr: 0.0001000, train loss: 1.30119, valid loss: 0.79397, acc: 72.5191.\n",
      "Wed May 18 15:15:49 2022 Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.28033, smth: 0.53250: 100%|████████████████████████████████████████████████████| 30/30 [00:12<00:00,  2.50it/s]\n",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.79      0.79      0.79        14\n",
      "          NV       0.84      0.80      0.82        20\n",
      "         BCC       0.80      0.94      0.86        47\n",
      "         BKL       1.00      0.75      0.86         4\n",
      "         SCC       0.62      0.42      0.50        12\n",
      "  AK_DF_VASC       0.91      0.85      0.88        34\n",
      "\n",
      "    accuracy                           0.82       131\n",
      "   macro avg       0.83      0.76      0.78       131\n",
      "weighted avg       0.82      0.82      0.82       131\n",
      "\n",
      "Wed May 18 15:16:02 2022 Epoch 2, lr: 0.0010000, train loss: 0.53250, valid loss: 0.52719, acc: 82.4427.\n",
      "Wed May 18 15:16:02 2022 Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17240, smth: 0.17371: 100%|████████████████████████████████████████████████████| 30/30 [00:12<00:00,  2.49it/s]\n",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.91      0.71      0.80        14\n",
      "          NV       0.89      0.80      0.84        20\n",
      "         BCC       0.77      0.94      0.85        47\n",
      "         BKL       0.75      0.75      0.75         4\n",
      "         SCC       0.78      0.58      0.67        12\n",
      "  AK_DF_VASC       0.88      0.82      0.85        34\n",
      "\n",
      "    accuracy                           0.82       131\n",
      "   macro avg       0.83      0.77      0.79       131\n",
      "weighted avg       0.83      0.82      0.82       131\n",
      "\n",
      "Wed May 18 15:16:15 2022 Epoch 3, lr: 0.0008536, train loss: 0.17371, valid loss: 0.70005, acc: 82.4427.\n",
      "Wed May 18 15:16:15 2022 Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.57589, smth: 0.09414: 100%|████████████████████████████████████████████████████| 30/30 [00:12<00:00,  2.49it/s]\n",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.86      0.86      0.86        14\n",
      "          NV       0.89      0.85      0.87        20\n",
      "         BCC       0.89      0.87      0.88        47\n",
      "         BKL       0.80      1.00      0.89         4\n",
      "         SCC       1.00      0.50      0.67        12\n",
      "  AK_DF_VASC       0.80      0.97      0.88        34\n",
      "\n",
      "    accuracy                           0.86       131\n",
      "   macro avg       0.87      0.84      0.84       131\n",
      "weighted avg       0.87      0.86      0.86       131\n",
      "\n",
      "Wed May 18 15:16:28 2022 Epoch 4, lr: 0.0005000, train loss: 0.09414, valid loss: 0.53898, acc: 86.2595.\n",
      "Wed May 18 15:16:28 2022 Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.29429, smth: 0.05201: 100%|████████████████████████████████████████████████████| 30/30 [00:12<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MEL       0.86      0.86      0.86        14\n",
      "          NV       0.89      0.80      0.84        20\n",
      "         BCC       0.86      0.89      0.88        47\n",
      "         BKL       0.80      1.00      0.89         4\n",
      "         SCC       1.00      0.58      0.74        12\n",
      "  AK_DF_VASC       0.84      0.94      0.89        34\n",
      "\n",
      "    accuracy                           0.86       131\n",
      "   macro avg       0.87      0.85      0.85       131\n",
      "weighted avg       0.87      0.86      0.86       131\n",
      "\n",
      "Wed May 18 15:16:41 2022 Epoch 5, lr: 0.0001464, train loss: 0.05201, valid loss: 0.54129, acc: 86.2595.\n"
     ]
    }
   ],
   "source": [
    "# print(model)\n",
    "auc_max = 0.\n",
    "\n",
    "model_file3 = os.path.join(args.model_dir, 'third_part_final.pth')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.init_lr)\n",
    "if args.use_amp:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "if DP:\n",
    "    model = nn.DataParallel(model)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.n_epochs - 1)\n",
    "scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "print(len(train_data), len(val_data))\n",
    "\n",
    "for epoch in range(1, args.n_epochs + 1):\n",
    "    print(time.ctime(), f'Epoch {epoch}')\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    val_loss, acc = val_epoch(model, valid_loader, mel_idx)\n",
    "\n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {(val_loss):.5f}, acc: {(acc):.4f}.'\n",
    "    print(content)\n",
    "    with open(os.path.join(args.log_dir, f'log_{args.kernel_type}.txt'), 'a') as appender:\n",
    "        appender.write(content + '\\n')\n",
    "\n",
    "    scheduler_warmup.step()    \n",
    "    if epoch==2: scheduler_warmup.step()\n",
    "\n",
    "torch.save(model.state_dict(), model_file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73f2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
